{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### These handcodes are written by me throughout my ML assignments, they are hand written functions in place of built in functions. Please do not submit these as your own. These are for educational reference only, thanks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding eigenvector by gradient ascent - SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x is the data matrix you are trying to find the eigenvectors on\n",
    "# w is your eigenvectors\n",
    "\n",
    "\n",
    "norm = lambda x: sum(x**2)**0.5     #normalization function\n",
    "\n",
    "\n",
    "def eigenvector(x,learn_rate):\n",
    "    w = np.random.normal(0,1,(2,1))     #initialize eigenvectors with random numbers\n",
    "    w = w / norm(w)                     #normalize the vectors, L2 norm\n",
    "    gradient = 2*x.dot(x.T).dot(w)      #for non-square SVD, we square it by multiplying data.dot(data.T) \n",
    "    w_new = w+learn_rate*gradient\n",
    "    counter = 1\n",
    "    while norm(w_new-w) > 0.001:\n",
    "        w = w_new\n",
    "        gradient = 2*x.dot(x.T).dot(w)\n",
    "        w_new = w+learn_rate*gradient\n",
    "        w_new = w_new / norm(w_new)\n",
    "        counter += 1\n",
    "    return w_new\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partial discreet fourier transform + window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample rate is your time domain data matrix\n",
    "\n",
    "def dft(sample_rate): \n",
    "    N=1600\n",
    "    F= np.zeros((N,N),dtype=complex)\n",
    "    y= 0.0\n",
    "    f= sample_rate/N\n",
    "    for f in range(N): #iterate through N=1600\n",
    "        for n in range(N): #double iterate to move n, while f keeps track\n",
    "            y = np.exp(-2J*np.pi*f*(n/N))\n",
    "            F[f][n] = y #now have row index[f] and column index[n] to append y to\n",
    "    return F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the second part of dft, with the hann window\n",
    "# data is your time domain data matrix\n",
    "\n",
    "def window(data):\n",
    "    N=1600\n",
    "    hann = np.hanning(N)\n",
    "    x_matrix= np.zeros((N,78))\n",
    "    x1 = data[0:N]\n",
    "    x_matrix[:,0] = x1*hann\n",
    "    for i in range(1,78): #i is index for row of matrix, row matrix=78\n",
    "        x2 = data[N//2:1600+N//2]*hann\n",
    "        #print(N//2,' ',1600+N//2)\n",
    "        x_matrix[:,i] = x2 #appends x2 coloumn-wise\n",
    "        N = N + 1600 #moves N so N not stagnant, since for function only moves i\n",
    "    return x_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverse DFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data is your frequency domain data matrix\n",
    "\n",
    "def reverse_window(data):\n",
    "    N=1600\n",
    "    x_list= []\n",
    "    first = data[0][:800]\n",
    "    x_list.extend(first)\n",
    "    for i in range(77):   #length of original window\n",
    "        x = data[i][N//2:N] + data[i+1][0:N//2]  #reverse hann window\n",
    "        x_list.extend(x)\n",
    "        \n",
    "    x_list.extend(data[77][:-800])\n",
    "    return x_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K means - 1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data is for single dimension 1d array data\n",
    "# k is number of clusters\n",
    "\n",
    "def k_means(data,k):\n",
    "    random_kmeans = np.random.choice(data,k) #generate dummy kmeans \n",
    "    counter = 1\n",
    "    points_list = np.zeros(len(data)) #empty list to hold distance points from k_means, for every data point\n",
    "    while counter < 100:  #define max iterations\n",
    "            for i in range(len(data)):  #iterate through each datapoint    \n",
    "                new_distance = np.sqrt((data[i]-random_kmeans)**2) #l2 norm distance for each data point\n",
    "                #print(new_distance)\n",
    "                points_list[i] = (np.argmin(new_distance)) #for each point, return index of min, result in (0,1)\n",
    "                #print(len(points_list))\n",
    "            mean1 = data[points_list == 0].mean() #get all indices with 0, mean\n",
    "            mean2 = data[points_list == 1].mean() #get all indices with 1, mean\n",
    "            random_kmeans = np.array([mean1, mean2]) #update dummy mean\n",
    "            counter = counter+1\n",
    "            k_len = len(data[points_list == 0]),len(data[points_list == 1])\n",
    "            var = sum((data[points_list == 0]-mean1)**2)/len(data[points_list == 0]),sum((data[points_list == 1]-mean2)**2)/len(data[points_list == 1])\n",
    "        \n",
    "    return random_kmeans, k_len, var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Mixture Model with Expectation Maximization - 1D GMM EM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stars_kmean is the initialized k_means, can be random variables also\n",
    "# data is your 1D matrix, only works with 1D matrix\n",
    "# vark is the variance derived from k_means \n",
    "# proba_k is the probability derived from k_means how many points belonging to each cluster\n",
    "\n",
    "\n",
    "def em(data, var_k, proba_k,stars_kmean):\n",
    "    #initialize mean,len,var,proba from kmean\n",
    "    counter = 1\n",
    "    old_mean = stars_kmean\n",
    "    new_mean = stars_kmean\n",
    "\n",
    "    while counter < 100:\n",
    "        #E-step, calculate posterior porbabilities with kmeans\n",
    "        old_mean = new_mean\n",
    "   \n",
    "        g_matrix = (1/np.sqrt((2*np.pi*var_k)))*np.exp(-1*(data[:,np.newaxis]-old_mean)**2/(2*var_k))\n",
    "        #print(g_matrix)\n",
    "        \n",
    "        den = (proba_k[0]*g_matrix[:,0])+(proba_k[1]*g_matrix[:,1])\n",
    "        den = np.array([den,den]).T\n",
    "        #print(den.shape)\n",
    "        uij = (proba_k*g_matrix)/den\n",
    "        #print(sum(uij))\n",
    "        \n",
    "        #M-step\n",
    "        uj = sum(uij*(data[:,np.newaxis]))/sum(uij)\n",
    "        #print(uj.shape)\n",
    "        pj = sum(uij)/2700\n",
    "        #print(pj.shape)\n",
    "        varj = sum((uij*(data[:,np.newaxis]-uj)**2))/sum(uij)\n",
    "        #print(varj.shape)\n",
    "        \n",
    "        #update parameters\n",
    "        var_k = varj\n",
    "        proba_k = pj\n",
    "        new_mean = uj\n",
    "        counter = counter+1\n",
    "    \n",
    "    return new_mean, proba_k, var_k\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterate and gather random consecutive blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_consecutive(iteration):\n",
    "    random = 90\n",
    "    y = []\n",
    "    counter=0\n",
    "    for i in range(iteration):\n",
    "        xr_block= xr[random:random+8,:]\n",
    "        xg_block= xg[random:random+8,:]\n",
    "        xb_block= xb[random:random+8,:]\n",
    "        \n",
    "        y.append(xr_block)\n",
    "        y.append(xg_block)\n",
    "        y.append(xb_block)\n",
    "          \n",
    "        random = random+25\n",
    "        counter = counter + 1\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal component analysis with whitening - PCA (whiten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_whiten(data):\n",
    "    zero_mean = data-data.mean()\n",
    "    \n",
    "    cov = zero_mean.T/len(zero_mean[0])\n",
    "    cov_matrix = zero_mean.dot(cov)\n",
    "    #print(cov_matrix.shape)\n",
    "    \n",
    "    v, w = np.linalg.eigh(cov_matrix)\n",
    "    \n",
    "    whiten = w.T/v**0.5\n",
    "    \n",
    "    return whiten, v, w\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Independent component analysis - ICA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# z is your reduced dimension data matrix \n",
    "\n",
    "\n",
    "def ica(Z, learn_rate):\n",
    "    N = 76800\n",
    "    W = np.identity(4)   #weight matrix\n",
    "    Y = W.dot(Z)  #estimated source\n",
    "    I = np.identity(4)\n",
    "    means = []\n",
    "    counter = 1\n",
    "    \n",
    "    while counter < 10000 :\n",
    "        \n",
    "        #print(W)\n",
    "        g = np.tanh(Y)\n",
    "        f = (np.power(Y,3))\n",
    "        W_delta = (N*(I)-g.dot(f.T))*(W)      \n",
    "        #print(W_delta)\n",
    "        \n",
    "        \n",
    "        W_new = W+(learn_rate*(W_delta))\n",
    "        #print(W_new)\n",
    "        Y_new = W_new.dot(Z)\n",
    "        \n",
    "        means.append((np.mean(W_new-W)))\n",
    "        \n",
    "        if np.allclose(W_new,W, rtol=1e-05):\n",
    "            break\n",
    "        \n",
    "        W = W_new\n",
    "        #print(W)\n",
    "        \n",
    "        Y = Y_new\n",
    "        #print(Y)\n",
    "        counter = counter + 1\n",
    "        \n",
    "        \n",
    "        \n",
    "    return W,Y, means, counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Signal to noise ratio - SNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#original is your original signal\n",
    "#recovered is your recovered signal\n",
    "#length is the original data signal length\n",
    "\n",
    "def snr_db(original,recovered,length):\n",
    "        num = sum(original**2)\n",
    "        #print(num)\n",
    "        den = np.sum((original[:length]-recovered)**2)\n",
    "        #print(den)\n",
    "        return 10*np.log10(num/den)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-negative matrix factorization -NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data is your non-negative (amplitude only) data matrix\n",
    "# vectors is how many vectors you want to keep (this is the reduced dimension)\n",
    "\n",
    "def nmf(data, vectors):\n",
    "    \n",
    "    X = data\n",
    "    x_len,y_len =data.shape\n",
    "    W = np.random.random((x_len, vectors))  #my w is 513x30\n",
    "    H = np.random.random((vectors, y_len))  #h.dot(w) = X (513,989), h need to be 30 x 989\n",
    "    lh = (X-W.dot(H)).T\n",
    "    rh = (X-W.dot(H))\n",
    "    obj = 0.5*np.trace(lh.dot(rh))\n",
    "    counter = 0\n",
    "    \n",
    "    while counter < 500:\n",
    "        \n",
    "        gamma = W/(W.dot(H).dot(H.T))\n",
    "        gamma_rh = (W.dot(H).dot(H.T))-(X.dot(H.T))\n",
    "        \n",
    "        W = W - np.multiply(gamma,gamma_rh)\n",
    "        #print(W)\n",
    "       \n",
    "        \n",
    "        #update rules\n",
    "        \n",
    "        W_rh = (X.dot(H.T))/(W.dot(H).dot(H.T))\n",
    "        W_new= np.multiply(W,W_rh)\n",
    "        W = W_new\n",
    "        \n",
    "        H_rh = (W.T.dot(X))/(W.T.dot(W).dot(H))\n",
    "        new_H = np.multiply(H,H_rh) \n",
    "        H = new_H\n",
    "        \n",
    "        counter = counter+1\n",
    "        #print(counter)\n",
    "        \n",
    "        lh = (X-W.dot(H)).T\n",
    "        rh = (X-W.dot(H))\n",
    "        obj_new = 0.5*np.trace(lh.dot(rh))\n",
    "        \n",
    "        if np.allclose(obj_new, obj, rtol=1e-04):      #objective break\n",
    "            break\n",
    "            \n",
    "        obj = obj_new\n",
    "        \n",
    "        \n",
    "    return W, H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NMF frequency extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mu_stft(data):\n",
    "    x_list= []\n",
    "    a1,a2,a3 = data.shape\n",
    "    counter= 1\n",
    "    for i in range(a3):\n",
    "        x = librosa.stft(data[:,0,i], n_fft = 64, hop_length=48, window='blackman')\n",
    "        y = librosa.stft(data[:,1,i], n_fft = 64, hop_length=48, window='blackman')\n",
    "        z = librosa.stft(data[:,2,i], n_fft = 64, hop_length=48, window='blackman')\n",
    "        \n",
    "        new_x = x[2:7,:]\n",
    "        #print(new_x.shape)\n",
    "        new_y = y[2:7,:]\n",
    "        new_z = z[2:7,:]\n",
    "        \n",
    "        cc = np.concatenate((new_x,new_y,new_z),axis=None)\n",
    "        #print(cc.shape)\n",
    "        \n",
    "        x_list.append(cc)\n",
    "       \n",
    "        counter = counter+1\n",
    "        \n",
    "    return x_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K nearest neighbors - KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNN(x_train, x_test, y_train):\n",
    "    x,y=x_train.shape\n",
    "    y_train = y_train.tolist()\n",
    "    min_val = []\n",
    "    distance = []\n",
    "    counter = 0\n",
    "    for i_1 in range(28):\n",
    "        for i in range(112):\n",
    "                dist = np.linalg.norm(x_test[:,i_1] - x_train[:,i])\n",
    "                distance.append(dist)\n",
    "                counter = counter+1\n",
    "                #print(distance)\n",
    "                holder = distance.index(min(distance))\n",
    "                #print(holder)\n",
    "        min_val.append(holder)\n",
    "        #print(min_val)\n",
    "        distance = []\n",
    "    y_pred = [y_train[val] for val in min_val] \n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_test,y_pred):\n",
    "    tracker = []\n",
    "    for i in range(len(y_test)):\n",
    "        if y_test[i] == y_pred[i]:\n",
    "            tracker.append(1)\n",
    "        else:\n",
    "            tracker.append(0)\n",
    "            \n",
    "        #print(tracker)\n",
    "    return sum(tracker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
